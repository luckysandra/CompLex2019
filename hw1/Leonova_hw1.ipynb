{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Я загрузила бинарник мануально</span>, потому что текущие версии RusVectores вешают ZIP-архивы, с которыми мне изнутри Питона неудобно работать.\n",
    "**Пункты 1 и 2**: Для начала импортируем необходимые библиотеки и построим векторную репрезентацию каждого слова. Поскольку пункт 2 требует от нас только сложить объекты array, этот шаг мы сделаем в той же ячейке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n"
     ]
    }
   ],
   "source": [
    "# YOU DON'T HAVE TO RUN THIS CELL\n",
    "# Я специально поместила этот импорт отдельно, потому как собираюсь залить свои данные\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def one_and_add():\n",
    "    # Сначала сделаем список [глагол, имя, частотность]\n",
    "    m = KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
    "    # Я использую модель НКРЯ SkipGram 2019-го года\n",
    "    with open('Leonova.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.readlines()\n",
    "    text = [i.replace('\\t', '_').strip('\\n').split('_') for i in text]\n",
    "    # Теперь обработаем векторы\n",
    "    verb = text[0][0] + '_VERB'\n",
    "    verb_vec = m.get_vector(verb) # вектор глагола: один на всех\n",
    "    c_text = []\n",
    "    for i in text:\n",
    "        noun = i[1] + '_NOUN'\n",
    "        if noun in m:\n",
    "            noun_vec = m.get_vector(noun) # вектор для каждого имени\n",
    "            add_vec = verb_vec + noun_vec # вектор кумулятивного контекста\n",
    "            c_text.append([i[0] + '_' + i[1], add_vec])\n",
    "    # Делаем твёрдую копию, чтобы файлы не потерялись\n",
    "    with open('clean_model.txt', 'w', encoding='utf-8') as f:\n",
    "        for line in c_text:\n",
    "            f.write(line[0] + '\\t')\n",
    "            for i in np.array(line[1]).tolist():\n",
    "                f.write(str(i) + '\\t')\n",
    "            f.write('\\n')\n",
    "    print('File saved')\n",
    "\n",
    "one_and_add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пункт 3**: Теперь создадим из файла матрицу совместной встречаемости и попробуем с ней поработать. Начнём с иерархической кластеризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_rep():\n",
    "    # Использую код из хэндаута со своей модификацией\n",
    "    array = []\n",
    "    d_mtr = {}\n",
    "    with open('clean_model.txt', 'r', encoding='utf-8') as f_input:\n",
    "        for line in f_input:\n",
    "            # Очистим строку и создадим вектор\n",
    "            line = line.strip().split('\\t')\n",
    "            vector = []\n",
    "            for item in line[1:]:\n",
    "                vector.append(float(item))\n",
    "            array.append(vector)\n",
    "            # Теперь сохраним элемент {фраза: массив}, она понадобится дальше\n",
    "            d_mtr[line[0]] = vector \n",
    "    # Создаём матрицу\n",
    "    X = np.array(array)\n",
    "    return X, d_mtr\n",
    "\n",
    "\n",
    "x, d_matrix = matrix_rep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Иерархическая кластеризация**: я ставлю threshold=1.154 и выбираю метод ward, потому что метод ward дал самые оптимально интерптерируемые результаты, а любое увеличение ведёт к единичному кластеру (плюс - этот threshold отражает визуальное распределение и приблизительно равен количеству значений в словарях).\n",
    "\n",
    "Когда я делала это без смены методов, количество кластеров = 49, что заставляет меня думать, что, возможно, с моими данными что-то не так. Я проверила данные, двигаясь с самого начала: постепенно повышая порог частотности. К сожалению, количество кластеров начало быть похожим на какое-либо естественное распределение (то есть стало хотя бы меньше 20) только после установки порога частотности в 50, который оставляет всего 11 сочетаний для работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество кластеров: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hierarchical(X):\n",
    "    # Использую код, представленный в хэндауте\n",
    "    z = hcluster.linkage(X, method='ward')\n",
    "    clusters = hcluster.fclusterdata(z, 1.15361)\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    print('Количество кластеров: %d' % len(set(clusters)))\n",
    "    plt.figure()\n",
    "    #plt.title('Иерархическая кластеризация для глагола закрыть')\n",
    "    #dn = hcluster.dendrogram(z)\n",
    "    plt.show()\n",
    "    return clusters\n",
    "\n",
    "hie_nabor = hierarchical(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**К-средних**: я решила пойти типологическим путём и вычислить значение кластеров, используя количество значений для глагола закрыть в русскоязычных словарях и значение глаголы close в Merriam-Webster (в этом словаре значения уточняются до очень тонких оттенков, мне показалось, что это будет полезно).\n",
    "\n",
    "Словарь Ожегова и Шведовой даёт 7 значений, из словаря Merriam-Webster я включила 1 значение (закрыть как в \"закрыть небо\"). Итого получился кластер из 8 значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 46, 7: 17, 4: 10, 3: 8, 5: 6, 6: 6, 2: 6, 0: 2})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "k_nabor = KMeans(n_clusters=8).fit(x)\n",
    "# чтобы оценить количество элементов в каждом кластере, воспользуемся Counter()\n",
    "print(collections.Counter(k_nabor.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пункты 4 и 5** я снова делаю вместе, потому что у меня есть словарь, в котором хранятся фразы.\n",
    "\n",
    "Прокомментирую не только внутрикодовыми комментариями, но и текстом: сначала я создаю словарь, в котором каждому значению кластера (т.е. номеру) соответствует список строк в нём (мне удобнее работать с таким объектом). Затем я вычисляю центр и по три элемента, стоящих близко к центру, причём оба метода (иерархический и к-средних) я обрабатываю в одной функции через параметр (опять же, вопрос моего удобства). Выходные данные - списки групп."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сортировка методом иерархической кластеризации: \n",
      "закрыть_губа, закрыть_глаз, закрыть_рот\n",
      "закрыть_спектакль, закрыть_мгновение, закрыть_тема\n",
      "закрыть_банк, закрыть_дыра, закрыть_труба\n",
      "закрыть_туча, закрыть_облако, закрыть_небо\n",
      "закрыть_рука, закрыть_колено, закрыть_нога\n",
      "\n",
      "\n",
      "Сортировка методом K-средних: \n",
      "закрыть_губа, закрыть_глазок, закрыть_рот\n",
      "закрыть_доступ, закрыть_позиция, закрыть_университет\n",
      "закрыть_клапан, закрыть_кран, закрыть_занавес\n",
      "закрыть_дыра, закрыть_луна, закрыть_солнце\n",
      "закрыть_кабак, закрыть_кабинет, закрыть_магазин\n",
      "закрыть_гроб, закрыть_стол, закрыть_постель\n",
      "закрыть_банка, закрыть_тетрадь, закрыть_папка\n"
     ]
    }
   ],
   "source": [
    "def comprehend(cluster):\n",
    "    clus_dict = {}\n",
    "    for j, c in enumerate(cluster):\n",
    "        n = x[j]\n",
    "        # Проверка на то, что объект уже в словаре\n",
    "        try:\n",
    "            value = clus_dict[str(c)]\n",
    "        except KeyError:\n",
    "            value = []\n",
    "        # Добавление строки в словарь\n",
    "        value.append(n)\n",
    "        clus_dict[str(c)] = value\n",
    "    return clus_dict\n",
    "\n",
    "\n",
    "def cent_clus(clusters, kmeans=False):\n",
    "    centers = []\n",
    "    # Вне зависимости от типа объекта, создаем словарь {номер кластера: строки}\n",
    "    if kmeans:\n",
    "        clus_dict = comprehend(clusters.labels_)\n",
    "        # Если объект kmeans, то высчитываем метрики центров\n",
    "        centers = clusters.cluster_centers_\n",
    "    else:\n",
    "        clus_dict = comprehend(clusters)\n",
    "    nabor = []\n",
    "    for k in clus_dict.keys():\n",
    "        try:\n",
    "            value = clus_dict[k]\n",
    "        except KeyError:\n",
    "            continue # на случай капризного kmeans\n",
    "        if len(value) <= 2:\n",
    "            continue # игнорируем слишком маленькие\n",
    "        value = np.array(value)\n",
    "        if kmeans:\n",
    "            # k-тый элемент - координата центра k-того кластера\n",
    "            mean = centers[int(k)]\n",
    "        else:\n",
    "            mean = np.mean(value)\n",
    "        # Высчитываем косинусное расстояние и берём только 3 ближайших\n",
    "        cls = {cosine(mean, j): j for j in value}\n",
    "        clst = sorted(cls, reverse=True)[:3]\n",
    "        # Запаковываем в список [[первый, второй, третий], [...]]\n",
    "        names = []\n",
    "        for j in clst:\n",
    "            for name, array in d_matrix.items():\n",
    "                # Ищем нужную строку в {слово: строка}\n",
    "                if np.all(cls[j] == array):\n",
    "                    names.append(name)\n",
    "        nabor.append(names)\n",
    "    return nabor\n",
    "\n",
    "# Создаём нужные нам списки\n",
    "hie_nabor = cent_clus(hie_nabor)\n",
    "k_nabor = cent_clus(k_nabor, kmeans=True)\n",
    "\n",
    "# Выводим\n",
    "print('Сортировка методом иерархической кластеризации: ')\n",
    "for i in hie_nabor:\n",
    "    print(', '.join(i))\n",
    "print('\\n')\n",
    "print('Сортировка методом K-средних: ')\n",
    "for i in k_nabor:\n",
    "    print(', '.join(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пункт 6**\n",
    "\n",
    "**Иерархическая кластеризация**: мне кажется, что этот метод сильнее зависит от выбранного имени для сочетания (в одну группу попадают явно разные значения _закрыть спектакль, закрыть театр, закрыть занавес_. Если говорить о иллюстративности, то этот метод мне кажется более когнитивно адекватным, чем k-средних: несмотря на наличие аутлаеров типа вышеуказанных, есть и очень точные наблюдения (например, специально мною выделенное значение закрыть_небо отражено отдельно, что мне представляется верным). В целом, как мне кажется, ему удалось охватить разные оттенки значений, хотя иногда его и \"путала\" принадлежность имени к определённому семантическому полю.\n",
    "\n",
    "**K-средних**: я долго думала, что в моих данных что-то не так, потому что получилось, конечно, что-то очень странное. Мне не кажется, что этот метод как-либо репрезентативно описывает мои данные. Прогнав алгоритм через данные несколько раз, я так и не смогла найти оптимальную репрезентацию, которая бы делила значения на осмысленные группы"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
